<h1 align="center">G-FedALA: Graph-Aware Federated Adaptive Local Aggregation</h1>

<p align="center">
  <b>Extending FedALA for Federated Graph Learning with Structure-Aware Aggregation</b>
</p>

<p align="center">
  <a href="#highlights">Highlights</a> â€¢
  <a href="#installation">Installation</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#method">Method</a> â€¢
  <a href="#experiments">Experiments</a> â€¢
  <a href="#citation">Citation</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/OpenFGL-Compatible-blue" alt="OpenFGL Compatible"/>
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red" alt="PyTorch"/>
  <img src="https://img.shields.io/badge/License-MIT-green" alt="License"/>
  <img src="https://img.shields.io/badge/Python-3.8+-yellow" alt="Python"/>
</p>

---

## Overview

**G-FedALA** (Graph-Aware Federated Adaptive Local Aggregation) is a novel federated graph learning algorithm that extends [FedALA](https://arxiv.org/abs/2212.01197) (AAAI 2023) to the graph domain. Built on the [OpenFGL](https://github.com/xkLi-Allen/OpenFGL) framework, G-FedALA addresses the unique challenges of federated learning on graph-structured data by integrating graph-structure awareness into the server-side aggregation process.

### Key Idea

In standard federated learning, statistical heterogeneity across clients degrades the performance of the global model. While FedALA addresses this through adaptive local aggregation on the client side, G-FedALA goes further by:

1. **Client-Side**: FedALA-based adaptive local aggregation using loss-driven, element-wise mixing of local and global model parameters. Clients compute graph embeddings locally and share them with the server.
2. **Server-Side**: Structure-aware aggregation that combines parameter similarity and graph-embedding similarity to weight client contributions.

---

## Highlights

| Feature | Description |
|---------|-------------|
| ğŸ”· **Graph-Aware Aggregation** | Utilizes client graph embeddings for structure-aware server aggregation |
| ğŸ”· **Adaptive Local Aggregation** | Learns per-parameter mixing weights for personalized initialization |
| ğŸ”· **Split Aggregation Strategy** | Backbone: Similarity-weighted, Head: Sample-sizeâ€“weighted |
| ğŸ”· **Warm-up Mechanism** | Initial rounds use graph+param distances, later rounds use only param distances |
| ğŸ”· **OpenFGL Compatible** | Seamlessly integrates with the OpenFGL benchmark framework |
| ğŸ”· **Graph-FL Ready** | Designed for graph classification tasks in the Graph-FL scenario |

---

## Installation

### Prerequisites

```bash
# Python 3.8+
# PyTorch 2.0+
# PyTorch Geometric
# Anaconda
# Git
```

### Install OpenFGL

Install from source:

```bash
git clone https://github.com/xkLi-Allen/OpenFGL.git
cd OpenFGL
pip install -e .
```
Or download the ZIP file from [OpenFGL GitHub](https://github.com/xkLi-Allen/OpenFGL) and extract it.

### Install Our Repository

```bash
git clone https://github.com/nisadefneAKSU/CS58010-Scalable-Learning-Systems.git
cd CS58010-Scalable-Learning-Systems
```

## Quick Start

Option A: Using Conda (Recommended for Windows)

```bash
# Create a new conda environment
conda create -n openfgl_env python=3.9
conda activate openfgl_env

# Install PyTorch (adjust CUDA version as needed)
# For CPU only:
conda install pytorch torchvision torchaudio cpuonly -c pytorch

# For GPU (CUDA 11.8 example):
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Install PyTorch Geometric
conda install pyg -c pyg

# Install other dependencies
pip install -r docs/requirements.txt
```
Option B: Using pip

```bash
# Create virtual environment
python -m venv openfgl_env
source openfgl_env/bin/activate  # On Windows: openfgl_env\Scripts\activate

# Install dependencies
pip install -r docs/requirements.txt
```

---

Add FedALA and G-FedALA Files

Place the following files in the OpenFGL repository structure:

### 3.1 Algorithm Implementation Folders
```
OpenFGL/
â”œâ”€â”€ openfgl/
â”‚   â”œâ”€â”€ flcore/
â”‚   â”‚   â”œâ”€â”€ fedala/              # â† Add this folder
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”‚   â””â”€â”€ server.py
â”‚   â”‚   â””â”€â”€ gfedala/             # â† Add this folder
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ client.py
â”‚   â”‚       â””â”€â”€ server.py
```

**What these contain:**
- `client.py`: Client-side training logic 
- `server.py`: Server-side aggregation logic
- `__init__.py`: Exports client and server classes

---

### 3.2 Configuration Files
```
OpenFGL/
â”œâ”€â”€ openfgl/
â”‚   â”œâ”€â”€ config.py      # â† Replace this file
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ basic_utils.py      # â† Replace this file
```

**Modified files:**
- `config.py`: Adds `"fedala"` and `"gfedala"` to supported algorithms
- `basic_utils.py`: Adds FedALA/G-FedALA client and server loading logic

---

### Main Training Script
```
OpenFGL/
â””â”€â”€ main.py                      # â† Add this file
```

**What it contains:**
- Dataset configuration
- Hyperparameter settings
- Training loop initialization

---

### Requirements File (Optional)
```
OpenFGL/
â””â”€â”€ docs/
    â””â”€â”€ requirements.txt         # â† Replace if using Windows-specific setup
```

---

## Verify File Structure

Your directory should look like this:
```
OpenFGL-main/
â”œâ”€â”€ main.py                      # Your training script
â”œâ”€â”€ openfgl/
â”‚   â”œâ”€â”€ flcore/
â”‚   â”‚   â”œâ”€â”€ config.py            # Modified
â”‚   â”‚   â”œâ”€â”€ fedala/              # NEW
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”‚   â””â”€â”€ server.py
â”‚   â”‚   â””â”€â”€ gfedala/             # NEW
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ client.py
â”‚   â”‚       â””â”€â”€ server.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ basic_utils.py       # Modified
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ requirements.txt         # Updated (optional)
â””â”€â”€ data/                        # Will be auto-generated
```

Edit main.py to set your experiment configuration (you can change other arguments to your liking):
```python
# Select algorithm
args.fl_algorithm = "fedala"  # or "gfedala"
```

## Method

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          G-FedALA Framework                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Client 1   â”‚     â”‚   Client 2   â”‚     â”‚   Client N   â”‚         â”‚
â”‚  â”‚              â”‚     â”‚              â”‚     â”‚              â”‚         â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚
â”‚  â”‚ â”‚   ALA    â”‚ â”‚     â”‚ â”‚   ALA    â”‚ â”‚     â”‚ â”‚   ALA    â”‚ â”‚         â”‚
â”‚  â”‚ â”‚ Module   â”‚ â”‚     â”‚ â”‚ Module   â”‚ â”‚     â”‚ â”‚ Module   â”‚ â”‚         â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚
â”‚  â”‚      â”‚       â”‚     â”‚      â”‚       â”‚     â”‚      â”‚       â”‚         â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚
â”‚  â”‚ â”‚  Local   â”‚ â”‚     â”‚ â”‚  Local   â”‚ â”‚     â”‚ â”‚  Local   â”‚ â”‚         â”‚
â”‚  â”‚ â”‚ Training â”‚ â”‚     â”‚ â”‚ Training â”‚ â”‚     â”‚ â”‚ Training â”‚ â”‚         â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚
â”‚  â”‚      â”‚       â”‚     â”‚      â”‚       â”‚     â”‚      â”‚       â”‚         â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚
â”‚  â”‚ â”‚  Graph   â”‚ â”‚     â”‚ â”‚  Graph   â”‚ â”‚     â”‚ â”‚  Graph   â”‚ â”‚         â”‚
â”‚  â”‚ â”‚Embedding â”‚ â”‚     â”‚ â”‚Embedding â”‚ â”‚     â”‚ â”‚Embedding â”‚ â”‚         â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                    â”‚                    â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                              â–¼                                      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                    â”‚      Server      â”‚                             â”‚
â”‚                    â”‚                  â”‚                             â”‚
â”‚                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                             â”‚
â”‚                    â”‚ â”‚  Compute     â”‚ â”‚                             â”‚
â”‚                    â”‚ â”‚  Distances   â”‚ â”‚                             â”‚
â”‚                    â”‚ â”‚ (Param+Graph)â”‚ â”‚                             â”‚
â”‚                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                             â”‚
â”‚                    â”‚        â”‚         â”‚                             â”‚
â”‚                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                             â”‚
â”‚                    â”‚ â”‚   Split      â”‚ â”‚                             â”‚
â”‚                    â”‚ â”‚ Aggregation  â”‚ â”‚                             â”‚ 
â”‚                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                             â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Client-Side: Adaptive Local Aggregation (ALA)

The ALA module learns element-wise mixing weights `w âˆˆ [0,1]` for head parameters:

```
Î¸_head = Î¸_local + (Î¸_global - Î¸_local) âŠ™ w
```

**Weight Learning Process:**
1. Initialize weights to 1 (full global)
2. Forward pass on local data with mixed parameters
3. Compute gradients: `âˆ‡w = âˆ‡Î¸ âŠ™ (Î¸_global - Î¸_local)`
4. Update: `w â† clip(w - Î· Â· âˆ‡w, 0, 1)`

**Three-Phase Strategy:**
- **Round 0**: Skip ALA (global = local)
- **Round 1**: Learn weights until convergence (up to 20 epochs)
- **Round 2+**: Single epoch refinement

### Server-Side: Graph-Aware Aggregation

**Distance Computation:**

1. **Parameter Distance** (using head parameters):
```
d_param(i) = âˆš(mean_k[(||Î¸_i^k - Î¸_g^k|| / ||Î¸_g^k||)Â²])
```

2. **Graph Distance** (during warm-up):
```
d_graph(i) = ||h_i - h_global|| / ||h_global||
```

**Aggregation Weight Calculation:**

```
logit_i = -[Î» Â· d_param(i) + (1-Î») Â· d_graph(i)]  # During warm-up
logit_i = -d_param(i)                              # After warm-up

Î±_i = softmax(logit / Ï„)
```

**Split Aggregation:**
- **Backbone**: Similarity-weighted aggregation using `Î±_i`
- **Head**: Sample-weighted aggregation (FedAvg-style)


> âš ï¸ **Current Limitation:** This implementation is hardcoded for the **GIN (Graph Isomorphism Network)** architecture. The backbone-neck-head split logic can be extended to other GNN models by modifying the layer detection functions.


### Hyperparameters

| Parameter | Symbol | Default | Description |
|-----------|--------|---------|-------------|
| `lambda_graph` | Î» | 0.5 | Balance between param (Î») and graph (1-Î») distances |
| `gala_temperature` | Ï„ | 1.0 | Softmax temperature for aggregation weights |
| `gala_warmup_rounds` | - | 5 | Rounds to incorporate graph similarity |
| `ala_eta` | Î· | 1.0 | Learning rate for ALA weight updates |
| `ala_data_ratio` | - | 0.8 | Fraction of local data used for ALA |

---

## Experiments

### Supported GNN Models

Currently, G-FedALA supports:

- **GIN** (Graph Isomorphism Network) âœ“

> **Note:** The current implementation uses hardcoded layer detection for the GIN architecture (`convs`, `batch_norms` for backbone; `lin1`, `batch_norm1`, `lin2` for head). While the backbone-neck-head split strategy is conceptually applicable to other GNN architectures, extending support requires modifying the `_is_backbone()`, `_is_neck_head()`, and `_head_params()` functions to match the target model's layer naming conventions.

### Extending to Other GNN Models

To add support for a new GNN architecture (e.g., GCN, GAT), modify the following functions:

**In `server.py`:**
```python
def _is_backbone(k: str) -> bool:
    """Modify to match your model's backbone layer names."""
    # Example for GCN:
    # return k.startswith("conv_layers.") or k.startswith("bn_layers.")
    return k.startswith("convs.") or k.startswith("batch_norms.")

def _is_neck_head(k: str) -> bool:
    """Modify to match your model's head layer names."""
    # Example for GCN:
    # return k.startswith("fc1.") or k.startswith("fc2.")
    return k.startswith("lin1.") or k.startswith("batch_norm1.") or k.startswith("lin2.")
```

**In `client.py`:**
```python
@staticmethod
def _head_params(model):
    """Modify to return head parameters for your model."""
    # Example for GCN:
    # return list(model.fc1.parameters()) + list(model.fc2.parameters())
    return (
        list(model.lin1.parameters()) +
        list(model.batch_norm1.parameters()) +
        list(model.lin2.parameters())
    )
```

### Running Experiments

#### Data Simulation

G-FedALA uses the **Label Skew** simulation mode from OpenFGL to create non-IID data distribution across clients:

```python
args.scenario = "graph_fl"
args.task = "graph_cls"
args.simulation_mode = "graph_fl_label_skew"  # Label-based non-IID partition
args.skew_alpha = 1.0 # a parameter not mentioned in OpenFGl/config.py but required to be passed to run the project. We predict that it should be same as the value of args.dirichlet_alpha
args.dirichlet_alpha = 1.0  # Controls heterogeneity (lower = more heterogeneous)
```

> **Note:** The `dirichlet_alpha` parameter controls the degree of label distribution skew. Lower values create more heterogeneous (non-IID) data distributions across clients.

#### Example Commands

```bash
# PROTEINS dataset with 10 clients
python main.py --dataset PROTEINS --num_clients 10 --fl_algorithm gfedala

# ENZYMES dataset with label skew partition
python main.py --dataset ENZYMES --num_clients 5 --simulation_mode graph_fl_label_skew

# COLLAB dataset (social network)
python main.py --dataset COLLAB --num_clients 10 --fl_algorithm gfedala

# Ablation: Without graph similarity (Î»=1.0)
python main.py --dataset MUTAG --fl_algorithm gfedala --lambda_graph 1.0
```

---

## File Structure

```
G-FedALA/
â”œâ”€â”€ gfedala/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py          # GFedALAClient implementation
â”‚   â””â”€â”€ server.py          # GFedALAServer implementation
â”œâ”€â”€ fedala/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py          # FedALAClient (baseline)
â”‚   â””â”€â”€ server.py          # FedALAServer (baseline)
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ gfedala_grid_search.py
â”‚   â”œâ”€â”€ get_best_params.py
â”‚   â””â”€â”€ ablation.py
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ gfedala_banner.png
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## Citation

If you find this work useful, please cite:

```bibtex
@misc{gfedala2024,
  title={G-FedALA: Graph-Aware Federated Adaptive Local Aggregation for Federated Graph Learning},
  author={Aksu N. D., Arkac C.},
  year={2024},
  note={Built on OpenFGL framework}
}
```

Please also cite the original FedALA paper:

```bibtex
@inproceedings{zhang2023fedala,
  title={FedALA: Adaptive Local Aggregation for Personalized Federated Learning},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={11237--11244},
  year={2023}
}
```

And the OpenFGL benchmark:

```bibtex
@misc{li2024openfgl,
  title={OpenFGL: A Comprehensive Benchmarks for Federated Graph Learning},
  author={Li, Xunkai and Zhu, Yinlin and Pang, Boyang and Yan, Guochen and Yan, Yeyu and Li, Zening and Wu, Zhengyu and Zhang, Wentao and Li, Rong-Hua and Wang, Guoren},
  year={2024},
  eprint={2408.16288},
  archivePrefix={arXiv}
}
```

---

## Acknowledgements

- [OpenFGL](https://github.com/xkLi-Allen/OpenFGL) - Comprehensive benchmark for federated graph learning
- [FedALA](https://github.com/TsingZ0/FedALA) - Original adaptive local aggregation method
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/) - Graph neural network library

---

<p align="center">
  <i>Built with â¤ï¸ for the Federated Graph Learning community</i>
</p>
